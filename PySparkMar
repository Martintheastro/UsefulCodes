##########################################################################################################################################################################################################################
                                                    THIS IS THE LIST OF USEFUL CODES I HAVE USED AT MY WORK. IT IS SORTED IN ASCENDING ORDER, NEWEST CODES ARE AT THE BOTTOM.
__________________________________________________________________________________________________________________________________________________________________________________________________________________________

****************************************************************************************** ### TABLE REPORTING ### *******************************************************************************************************
from pyspark.sql.functions import *
TableNames = ["",""]
results=[]
for tableName in TableNames:
    df=spark.read.table(tableName)
    Mdate=df.agg(max('LastModifiedDate')).collect()[0][0]
    count=spark.table(tableName).count()
    results.append((tableName,count,Mdate))

Table1 = 'filter_Volume_Opp__History'    # the table has different date column name
df_Table1 = spark.read.table(Table1)
countT1 = df_Table1.count()
Mdate_T1=df_Table1.agg(max('CreatedDate')).collect()[0][0]
results.append((Table1,countT1,Mdate_T1))

results_df = spark.createDataFrame(results,["TableName","RowCount",'DateMax'])\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()
# shows a table that contains the name of table, row count and max date 

*********************************************************************************************************************************************************************************************************
******************************************************************** ### FINDING MAX DATE FOR INCREMENTAL REFRESH ### ***********************************************************************************

from pyspark.sql.functions import *

TableNames_modifedDate = ["",""]
results=[]
# start_date = '2024-07-01'

for tableName in TableNames_modifedDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('LastModifiedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('LastModifiedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")

TableNames_createDate = ['']
for tableName in TableNames_createDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('CreatedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('CreatedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")
# Saves as a table with only one row which contains the max date found in the data of last refresh
**************************************************************************************************************************************************************************************************************
************************************************************************** LIST COLUMNS IN THE DATAFRAME *****************************************************************************************************
for field in df.schema.fields:
    print(field.name +" , "+str(field.dataType))
print(df.columns)

*************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT OF EACH OF THE TABLES IN LAKEHOUSE ***************************************************************************************
# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()

# Filter tables starting with "staging"
staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]
results=[]

for table_name in staging_tables:
    full_table_name=f"{database_name}.{table_name}"
    trimmed_table_name=table_name[8:]
    count=spark.table(full_table_name).count()
    results.append((trimmed_table_name,count))
    

results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .orderBy(col('RowCount').asc())\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()

****************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT ADJUSTED ********************************************************************************************************************

# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "

# Getting table names from the Lakehouse:
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()
tables=[table["tableName"] for table in tables]

# Filter tables starting with "staging"
# staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]

results=[]

# Calculate row count for each of the tables and append the results into an array with table name and row count:
for table_name in tables:
    # full_table_name=f"{database_name}.{table_name}"
    # trimmed_table_name=table_name[8:]
    count=spark.table(table_name).count()
    results.append((table_name,count))
    
# Create dataframe form the array:
results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .withColumn('RowCount',format_number('RowCount',0))


# Splitting Table Name to create more readable report of row count in the Lakehouse
split_col = split(results_df['TableName'], 'SPLITTER')
trimmed_df = results_df.withColumn('Stage', split_col.getItem(0))\
    .withColumn('TableName', split_col.getItem(1))\
    .orderBy(col('TableName').asc())\
    .select('Stage','TableName','RowCount')
    
display(trimmed_df)

***********************************************************************************************************************************************************************************************************************
**************************************************** THOUSANDS SEPARATOR ISSUE ****************************************************************************************************************************************
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, when, length, expr

# MULTIPLE COLUMNS THAT START WITH DAY_
day_columns = [col_name for col_name in df.columns if col_name.startswith('DAY')]


def process_decimal_column(col_name):
    parts = split(col(col_name), "\.")
    is_negative = col(col_name).startswith("-")
    no_decimal = parts.getItem(1).isNull()
    
    before_dot = when(is_negative,parts.getItem(0).cast("int") * -1000)\
                 .otherwise(parts.getItem(0).cast("int") * 1000)
                 
    after_dot_value = when(length(parts.getItem(1)) >= 3, parts.getItem(1).substr(1,3).cast("int"))\
                      .when(length(parts.getItem(1)) == 1, parts.getItem(1).cast("int") * 100)\
                      .when(length(parts.getItem(1)) == 2, parts.getItem(1).cast("int") * 10)\
                      .otherwise(parts.getItem(1).cast("int"))

    
    after_dot = when(is_negative, -1 * after_dot_value).otherwise(after_dot_value)
    
    # Combining the before and after dot parts based on negativity
    result = when(no_decimal, col(col_name)).otherwise(before_dot + after_dot)
    
    return result



# Using a for loop to apply the function to each 'DAY' column
for col_name in day_columns:
    df = df.withColumn(col_name + "_processed", process_decimal_column(col_name))
# Select the modified columns along with any other columns you need
# df = df.select(*modifications)

# Show the updated DataFrame

display(df)

*************************************************************************************************************************************************************************************************************************
********************************************** NEW CODE FOR CREATING SILVER TABLE IN PIPELINE RUN ***********************************************************************************************************************
from pyspark.sql.functions import *

# Change the name of tables if necessary: staging table is from pipeline, Silver table is adjusted table, Grouped table is table for teporting and only one for all tables:
var_StagingName = "staging_"+var_TableName
var_TableNameSilver = "silver_"+var_TableName
var_TableNameGrouped = "report__load"
# var_Timestamp = current_timestamp()

# reading the staging table from the pipeline and adding ExtractTimestampGMT to the dataframe with current timestamp and saving the table as lkh_silver_ table:
df=spark.read.table(var_StagingName)
# number of rows loaded:
var_dfCount = df.count()
# adding ExtractTimestampUTC to staging table and save it as silver table:
df.withColumn('ExtractTimestampUTC',lit(var_StartTime))\
    .write.mode("overwrite").format("delta").option("overwriteSchema", "true").save(f"Tables/{var_TableNameSilver}")

# UPDATING REPORTING TABLE
# creating Dataframe from variables and schema:
var_Duration = ""
report_data=[(var_StartTime,var_WorkspaceID,var_PipelineName,var_PipelineRunID,var_TableName,var_dfCount,var_StartTime,var_EndTime,var_Duration)]
columns=["ExtractDate","WorkspaceID","PipelineName","PipelineRunID","TableName","NumOfRowsLoaded","CopyActivityStartTime","CopyActivityEndTime","CopyActivityDuration_sec"]

# creating dataframe and adjusting the column format and calculating duration of load:
df=spark.createDataFrame(report_data,columns)\
    .withColumn('ExtractDate', to_date('ExtractDate'))\
    .withColumn('CopyActivityStartTime',col('CopyActivityStartTime').cast('timestamp'))\
    .withColumn('CopyActivityEndTime',col('CopyActivityEndTime').cast('timestamp'))\
    .withColumn('CopyActivityDuration_sec',unix_timestamp('CopyActivityEndTime')-unix_timestamp('CopyActivityStartTime')) # works for midnight timestamps
    

# Appending the dataframe to the report table:
df.write.mode("append").format("delta").partitionBy("ExtractDate").save(f"Tables/{var_TableNameGrouped}")

# Deleting staging table:
spark.sql(f"DROP TABLE IF EXISTS {var_StagingName}")
**************************************************************************************************************************************************************************************************************************
********************************************************* COMPARING MIN AND MAX VALUES OF TWO TABLES IN ONE COLUMN, CUNTING ROWS *****************************************************************************************

from pyspark.sql.functions import *

# comparing stat for two tables (max, min values for date columns; row count) into one report table:

var_ARtable = "staging_"
var_INCRtable = "staging_"
var_Timestamp = current_timestamp()
incr_column_name = 'columnName'

tables = [var_ARtable,var_INCRtable]
results=[]

for table_name in tables:
    ar_df = spark.read.table(table_name).select(incr_column_name) # just loading one columne for each table, we are only interested for this one
    ar_df = ar_df.withColumn(incr_column_name,to_date(incr_column_name)) # to have column for each table in the same data type
    minHLoadTS_row = ar_df.agg(min(incr_column_name).alias('MinValue')).first() # finding min value, result is dataframe
    maxHLoadTS_row = ar_df.agg(max(incr_column_name).alias('MaxValue')).first() # finding max value, result is dataframe
    min_value=minHLoadTS_row['MinValue'] # turning into value from dataframe
    max_value=maxHLoadTS_row['MaxValue'] # turning into value from dataframe
    row_count = ar_df.count() #counting number of rows
    results.append((table_name,min_value,max_value,row_count)) # appending each result for each table into array

# creating reporting dataframe:

columns_ar = ["TableName","minHLoadTS","maxHLoadTS","RowCount"]
results_df=spark.createDataFrame(results,columns_ar).withColumn('ReportDate',lit(var_Timestamp))

results_df.write.mode("append").format("delta").save(f"Tables/IncrementalReporting")

**************************************************************************************************************************************************************************************************************************
****************************************************************************IMPROVED CODE TO CREATE INGESTION REPORTING IN LAKEHOUSE**************************************************************************************

# Use when moving data from lakehouse to warehouse (uses silver tables only, no staging)

var_TableNameReport = 'report_table'
var_DatabaseName = "lakehouse name"
tables = spark.sql(f"SHOW TABLES IN {var_DatabaseName}").collect()


# Filter tables starting with "silver_" - it collect all tables that are in lakehouse and test it if it starts with "silver":
silver_tables = [table["tableName"] for table in tables if table["tableName"].startswith("silver")]
results=[]

# for each table collected in results collection, trim the name of the table to remove 'silver_' and get just the original name of the table:
for table_name in silver_tables:
    full_table_name=f"{database_name}.{table_name}"
    trimmed_table_name=table_name[7:]
  # test the original name of the table from the results collection if it is equal to table name from the pipeline var_TableName:  
    if trimmed_table_name == var_TableName:
        
    
        var_Duration = "" # variable set to be able to calculate later in the code
        # counting the rows of the table in the lakehose ( we use silver_ name table to be able to count the rows, because this is how the table is saved in lakehose)
        var_dfCount=spark.table(table_name).count()
         # creating Dataframe from variables that belong to the pipeline's var_TableName:
        report_data=[(var_StartTime,var_WorkspaceID,var_PipelineName,var_PipelineRunID,var_TableName,var_dfCount,var_StartTime,var_EndTime,var_Duration)]
        columns=["ExtractDate","WorkspaceID","PipelineName","PipelineRunID","TableName","NumOfRowsLoaded","CopyActivityStartTime","CopyActivityEndTime","CopyActivityDuration_sec"]

        # creating dataframe and adjusting the column format and calculating duration of load:
        df=spark.createDataFrame(report_data,columns)\
            .withColumn('ExtractDate', to_date('ExtractDate'))\
            .withColumn('CopyActivityStartTime',col('CopyActivityStartTime').cast('timestamp'))\
            .withColumn('CopyActivityEndTime',col('CopyActivityEndTime').cast('timestamp'))\
            .withColumn('CopyActivityDuration_sec',unix_timestamp('CopyActivityEndTime')-unix_timestamp('CopyActivityStartTime')) # works for midnight timestamps
            # Appending the dataframe to the report table:
        df.write.mode("append").format("delta").partitionBy("ExtractDate").save(f"Tables/{var_TableNameReport}")
**********************************************************************************************************************************************************************************************
*********************************************************************  DAY CHECK CODE ********************************************************************************
from pyspark.sql.functions import current_date, dayofmonth
from datetime import datetime
from pyspark.sql.functions import lit

# Current_day is variable from a function to return the name of the day (Sunday, Monday, Tuesday...)

current_day = datetime.now().strftime('%A')

# this is the array of days in the week, when we want to run the pipeline
run_days=("Sunday")

# Check if the current day is in the array, which means today is a day to run the pipeline:
# if today is not the day the else will come which will end in the error (we dont have df defined). The error will is going to stop the pipeline
if current_day in run_days:
    print(f"The day is in the the days of run for the month")
else:
    df.write.mode("append")

*************************************************************************************************************************************************************************************************
********************************************* SOME NICE CODE TO GROUP BY FIELDS BASED ON YEAR, MONTH, DAY TRANSFORMATIONS **************************************************************************

from pyspark.sql.functions import date_format, year, month, dayofmonth, col

df_2=df.withColumn("SystemModstam_hour", date_format("SystemModstamp","HH"))\
    .withColumn("SystemModstam_year", year(col("SystemModstamp")))\
    .withColumn("SystemModstam_month", month(col("SystemModstamp")))\
    .withColumn("SystemModstam_dayofmonth", dayofmonth(col("SystemModstamp")))\
    .filter((col("SystemModstam_month")>9)&(col("SystemModstam_dayofmonth")>1))\
    .groupBy("SystemModstam_hour")\
    .count()
display(df_2)
*******************************************************************************************************************************************************************************************************
*************************************************** INCREMENTAL REFRESH *******************************************************************************************************************************

# incoming table from the pipeline, that will cause duplicate record:
append_df=spark.read.table(var_IncomingTable)

# original table to which incoming table will be appended:
df=spark.read.table(var_MainTable)

# to get simple table with Id and count columns only from the incoming table:
group_df = append_df.groupBy('Id').count()

# count column will be added to the main sink table and only unique records will be saved in the lakehouse:
# NULL values in the 'count' column mean, that the 'Id' is not found in the incoming table (we are removing future duplicates, old Ids, that will be replace by new ids)

join_df= df.join(group_df,on='Id',how='left')\
    .filter(col('count').isNull())\
    .drop('count')

# to join two dataframes they must have the same columns, we are appending incoming table wit new ids to the original table, where we remoed the old ids:
join_df=join_df.union(append_df)

# saving the new table:

*********************************************************************************************************************************************************************************************************
**************************************************************************************** EXCEL TABLE LOADING ********************************************************************************************

import os,re

def adjust_header(name):
    # Replace unwanted characters with an empty string
    new_name=re.sub(r'[\[\]\-\@\.\\]', '', name)
    new_name2=re.sub(r'%','Percentage',new_name)
    return re.sub(r'[(\)\ ]','_',new_name2)


# Function to convert first row into header
def use_first_row_as_header(df):
    # Collect the first row (this is the header row) as a list:
    first_row = df.first().asDict()
    
    # Extract column names from the first row
    new_columns = list(first_row.values())
    
    # Sanitize the new column names
    new_columns = [adjust_header(col) for col in new_columns]
    
    # Create new DataFrame from second row onwards(filter all columns except first):
    df_without_first = df.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])
    
    # Convert to DataFrame and set new column names
    new_df = spark.createDataFrame(df_without_first, schema=new_columns)
    
    return new_df

# Complete file path

file_name='OnHold'
var_TableName= 'PortfolioReport_'

file_path=os.path.join('abfss://DS_Qlik@onelake.dfs.fabric.microsoft.com/lkh_QLIK_PMO.Lakehouse/Files/PortfolioReport/', file_name)
    # Check if path is a file and if it has the .parquet extension
    
        # Load the parquet file into a DataFrame
df = spark.read.parquet(file_path)
        # Use the first row as the header and sanitize it
new_df=use_first_row_as_header(df)

        # For demonstration, print the DataFrame schema and show the first few rows
print(f"Loaded {file_name}")
new_df.write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}{file_name}")


#-------------------------------------------------------------------------------------------------------------------------------------------#
#                                                           FUNCTIONS                                                                       #
#-------------------------------------------------------------------------------------------------------------------------------------------#

# When header contains characters that are not accepted by lakehouse:
def adjust_header(name):
    # Replace unwanted characters with an empty string
    new_name=re.sub(r'[\[\]\-\@\.\/]', '', name)
    new_name2=re.sub(r'%','Percentage',new_name)
    return re.sub(r'[(\)\ ]','_',new_name2)

# Function to convert first row into header (the Excel file does not have header)
def use_first_row_as_header(df):
    # Collect the first row (this is the header row) as a list:

    index_df = df.withColumn("index", monotonically_increasing_id())
    index_df=index_df.filter(index_df["index"] == header_row_number).drop("index")
    first_row = index_df.first().asDict()
    
    # Extract column names from the first row
    new_columns = list(first_row.values())
    
    # Sanitize the new column names
    new_columns = [adjust_header(col) for col in new_columns]
    
    # Create new DataFrame from second row onwards(filter all columns except first):
    df_without_first = df.rdd.zipWithIndex().filter(lambda x: x[1] > header_row_number).map(lambda x: x[0])
    
    # Convert to DataFrame and set new column names
    new_df = spark.createDataFrame(df_without_first, schema=new_columns)
    
    return new_df

#-------------------------------------------------------------------------------------------------------------------------------------------#
#                                                       PROCEED DATAFRAME                                                                   #
#-------------------------------------------------------------------------------------------------------------------------------------------#

# Use the first row as the header and sanitize it
new_df=use_first_row_as_header(df)

# For demonstration, print the DataFrame schema and show the first few rows
print(f"Loaded {file_name}")

#Save the table
new_df.write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}{file_name}")
