**************** CODE APPLIED AFTER COPY PIPELINE TO ADD TIMESTAMP AND REPORTING OF LOADED ROWS***********************************************************************.
#######DEFINED IN SET VARIABLE ACTIVITY IN PIPELINE:
var_TableName = 'no-table'
var_PipelineID = 'no-id'
var_PipelineName = 'no-name'
var_WorkspaceID = 'no-id'
var_StartTime = 'no-start-time'
var_EndTime = 'no-end-time'


from pyspark.sql.functions import *

# Change the name of tables if necessary: staging table is from pipeline, Silver table is adjusted table, Grouped table is table for teporting and only one for all tables:
var_StagingName = "staging_"+var_TableName
var_TableNameSilver = "silver_"+var_TableName
var_TableNameGrouped = "report_dppm_load"
# var_Timestamp = current_timestamp()

# reading the staging table from the pipeline and adding ExtractTimestampGMT to the dataframe with current timestamp and saving the table as lkh_silver_ table:
df=spark.read.table(var_StagingName)\
    .withColumn('ExtractTimestampUTC',lit(var_StartTime))\   # .withColumn('ExtractTimestampGMT',current_timestamp())\
    .write.mode("overwrite").format("delta").save(f"Tables/{var_TableNameSilver}")

# UPDATING REPORTING TABLE
# loading the silver table to gorup by timestamp and count number of rows and also adding name of the table and timestamps, renaming "count" column to NumOfRowsLoaded
# and reordering columns with 'selec' function and appending the dataframe as delta table to TableNameGrouped:
df = spark.read.table(var_TableNameSilver)\
    .withColumn('ExtractTimestampUTC', to_date('ExtractTimestampUTC'))\
    .withColumnRenamed('ExtractTimestampUTC','ExtractDate')\
    .groupBy("ExtractDate")\
    .count()\
    .withColumn("TableName",lit(var_TableNameSilver))\
    .withColumn('PipelineRunID',lit(var_PipelineID))\
    .withColumn('CopyActivityStartTime',lit(var_StartTime))\
    .withColumn('CopyActivityEndTime',lit(var_EndTime))\
    .withColumn('CopyActivityStartTime',date_format('CopyActivityStartTime','HH:mm:ss'))\
    .withColumn('CopyActivityEndTime',date_format('CopyActivityEndTime','HH:mm:ss'))\
    .withColumn('CopyActivityStartTime',to_timestamp('CopyActivityStartTime'))\
    .withColumn('CopyActivityEndTime',to_timestamp('CopyActivityEndTime'))\
    .withColumn('CopyActivityDuration_sec',unix_timestamp('CopyActivityEndTime')-unix_timestamp('CopyActivityStartTime'))\
    .withColumn('PipelineName',lit(var_PipelineName))\
    .withColumn('WorkspaceID',lit(var_WorkspaceID))\
    .withColumnRenamed("count","NumOfRowsLoaded")\
    .select("ExtractDate","WorkspaceID","PipelineName","PipelineRunID","TableName","NumOfRowsLoaded","CopyActivityStartTime","CopyActivityEndTime","CopyActivityDuration_sec")

df.write.mode("append").format("delta").partitionBy("ExtractDate").save(f"Tables/{var_TableNameGrouped}")

*********************************************************************************************************************************************************************************************************
***************************************************************************** ### TABLE REPORTING ### ***************************************************************************************************
from pyspark.sql.functions import *
TableNames = ["filter_Year_Summary_Opp__c","filter_Volume_Opp__c"]
results=[]
for tableName in TableNames:
    df=spark.read.table(tableName)
    Mdate=df.agg(max('LastModifiedDate')).collect()[0][0]
    count=spark.table(tableName).count()
    results.append((tableName,count,Mdate))

Table1 = 'filter_Volume_Opp__History'    # the table has different date column name
df_Table1 = spark.read.table(Table1)
countT1 = df_Table1.count()
Mdate_T1=df_Table1.agg(max('CreatedDate')).collect()[0][0]
results.append((Table1,countT1,Mdate_T1))

results_df = spark.createDataFrame(results,["TableName","RowCount",'DateMax'])\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()
# shows a table that contains the name of table, row count and max date 

*********************************************************************************************************************************************************************************************************
******************************************************************** ### FINDING MAX DATE FOR INCREMENTAL REFRESH ### ***********************************************************************************

from pyspark.sql.functions import *

TableNames_modifedDate = ["filter_Year_Summary_Opp__c","filter_Volume_Opp__c"]
results=[]
# start_date = '2024-07-01'

for tableName in TableNames_modifedDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('LastModifiedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('LastModifiedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")

TableNames_createDate = ['filter_Volume_Opp__History']
for tableName in TableNames_createDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('CreatedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('CreatedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")
# Saves as a table with only one row which contains the max date found in the data of last refresh
**************************************************************************************************************************************************************************************************************
************************************************************************** LIST COLUMNS IN THE DATAFRAME *****************************************************************************************************
for field in df.schema.fields:
    print(field.name +" , "+str(field.dataType))

*************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT OF EACH OF THE TABLES IN LAKEHOUSE ***************************************************************************************
# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()

# Filter tables starting with "staging"
staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]
results=[]

for table_name in staging_tables:
    full_table_name=f"{database_name}.{table_name}"
    trimmed_table_name=table_name[8:]
    count=spark.table(full_table_name).count()
    results.append((trimmed_table_name,count))
    

results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .orderBy(col('RowCount').asc())\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()

****************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT ADJUSTED ********************************************************************************************************************

# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "

# Getting table names from the Lakehouse:
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()
tables=[table["tableName"] for table in tables]

# Filter tables starting with "staging"
# staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]

results=[]

# Calculate row count for each of the tables and append the results into an array with table name and row count:
for table_name in tables:
    # full_table_name=f"{database_name}.{table_name}"
    # trimmed_table_name=table_name[8:]
    count=spark.table(table_name).count()
    results.append((table_name,count))
    
# Create dataframe form the array:
results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .withColumn('RowCount',format_number('RowCount',0))


# Splitting Table Name to create more readable report of row count in the Lakehouse
split_col = split(results_df['TableName'], 'SPLITTER')
trimmed_df = results_df.withColumn('Stage', split_col.getItem(0))\
    .withColumn('TableName', split_col.getItem(1))\
    .orderBy(col('TableName').asc())\
    .select('Stage','TableName','RowCount')
    
display(trimmed_df)
