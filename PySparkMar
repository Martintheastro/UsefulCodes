**************** CODE APPLIED AFTER COPY PIPELINE TO ADD TIMESTAMP AND REPORTING OF LOADED ROWS***********************************************************************.
# Variables set in the pipeline:
var_TableName = 'no-table'
var_PipelineRunID = 'no-id'
var_PipelineName = 'no-name'
var_WorkspaceID = 'no-id'
var_StartTime = 'no-start-time'
var_EndTime = 'no-end-time'


from pyspark.sql.functions import *

# Change the name of tables if necessary: var_TableNameGrouped table is for reporting purposes:
var_StagingName = "staging_"+var_TableName
var_TableNameSilver = "silver_"+var_TableName
var_TableNameGrouped = "report_dppm_load"
# var_Timestamp = current_timestamp()

# reading the staging table from the pipeline activity and adding ExtractTimestampUTC column with var_StartTime to the dataframe and saving the table as silver_ table:
df=spark.read.table(var_StagingName)\
    .withColumn('ExtractTimestampUTC',lit(var_StartTime))\   # .withColumn('ExtractTimestampGMT',current_timestamp())\
    .write.mode("overwrite").format("delta").option("overwriteSchema", "true").save(f"Tables/{var_TableNameSilver}")

# UPDATING REPORTING TABLE
# loading the silver table to gorup by timestamp and count number of rows and also adding variables from the pipeline for the reporting
# and reordering columns with 'select' function and appending the dataframe as delta table to TableNameGrouped. ExtractTimestampUTC is changed to date format only to be able to group by one date:
df = spark.read.table(var_TableNameSilver)\
    .withColumn('ExtractTimestampUTC', to_date('ExtractTimestampUTC'))\
    .withColumnRenamed('ExtractTimestampUTC','ExtractDate')\
    .groupBy("ExtractDate")\
    .count()\
    .withColumn("TableName",lit(var_TableName))\
    .withColumn('PipelineRunID',lit(var_PipelineRunID))\
    .withColumn('CopyActivityStartTime',lit(var_StartTime))\
    .withColumn('CopyActivityEndTime',lit(var_EndTime))\
    .withColumn('CopyActivityStartTime',date_format('CopyActivityStartTime','HH:mm:ss'))\
    .withColumn('CopyActivityEndTime',date_format('CopyActivityEndTime','HH:mm:ss'))\
    .withColumn('CopyActivityStartTime',to_timestamp('CopyActivityStartTime'))\
    .withColumn('CopyActivityEndTime',to_timestamp('CopyActivityEndTime'))\
    .withColumn('CopyActivityDuration_sec',unix_timestamp('CopyActivityEndTime')-unix_timestamp('CopyActivityStartTime'))\
    .withColumn('PipelineName',lit(var_PipelineName))\
    .withColumn('WorkspaceID',lit(var_WorkspaceID))\
    .withColumnRenamed("count","NumOfRowsLoaded")\
    .select("ExtractDate","WorkspaceID","PipelineName","PipelineRunID","TableName","NumOfRowsLoaded","CopyActivityStartTime","CopyActivityEndTime","CopyActivityDuration_sec")

df.write.mode("append").format("delta").partitionBy("ExtractDate").save(f"Tables/{var_TableNameGrouped}")

*********************************************************************************************************************************************************************************************************
***************************************************************************** ### TABLE REPORTING ### ***************************************************************************************************
from pyspark.sql.functions import *
TableNames = ["filter_Year_Summary_Opp__c","filter_Volume_Opp__c"]
results=[]
for tableName in TableNames:
    df=spark.read.table(tableName)
    Mdate=df.agg(max('LastModifiedDate')).collect()[0][0]
    count=spark.table(tableName).count()
    results.append((tableName,count,Mdate))

Table1 = 'filter_Volume_Opp__History'    # the table has different date column name
df_Table1 = spark.read.table(Table1)
countT1 = df_Table1.count()
Mdate_T1=df_Table1.agg(max('CreatedDate')).collect()[0][0]
results.append((Table1,countT1,Mdate_T1))

results_df = spark.createDataFrame(results,["TableName","RowCount",'DateMax'])\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()
# shows a table that contains the name of table, row count and max date 

*********************************************************************************************************************************************************************************************************
******************************************************************** ### FINDING MAX DATE FOR INCREMENTAL REFRESH ### ***********************************************************************************

from pyspark.sql.functions import *

TableNames_modifedDate = ["filter_Year_Summary_Opp__c","filter_Volume_Opp__c"]
results=[]
# start_date = '2024-07-01'

for tableName in TableNames_modifedDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('LastModifiedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('LastModifiedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")

TableNames_createDate = ['filter_Volume_Opp__History']
for tableName in TableNames_createDate:
    temp_df=spark.read.table(tableName)
    incr_date=temp_df.agg(max('CreatedDate')).collect()[0][0]
    var_TableName = 'incr_'+tableName
    filter_df =temp_df.filter(col('CreatedDate') >= lit(incr_date)).limit(1)\
        .write.mode("overwrite").format("delta").save(f"Tables/{var_TableName}")
# Saves as a table with only one row which contains the max date found in the data of last refresh
**************************************************************************************************************************************************************************************************************
************************************************************************** LIST COLUMNS IN THE DATAFRAME *****************************************************************************************************
for field in df.schema.fields:
    print(field.name +" , "+str(field.dataType))

*************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT OF EACH OF THE TABLES IN LAKEHOUSE ***************************************************************************************
# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()

# Filter tables starting with "staging"
staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]
results=[]

for table_name in staging_tables:
    full_table_name=f"{database_name}.{table_name}"
    trimmed_table_name=table_name[8:]
    count=spark.table(full_table_name).count()
    results.append((trimmed_table_name,count))
    

results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .orderBy(col('RowCount').asc())\
    .withColumn('RowCount',format_number('RowCount',0))
results_df.show()

****************************************************************************************************************************************************************************************************************
************************************************************************ ROW COUNT ADJUSTED ********************************************************************************************************************

# ROW COUNT of staging tables only in the Lakehouse

from pyspark.sql.functions import *
database_name = " "

# Getting table names from the Lakehouse:
tables = spark.sql(f"SHOW TABLES IN {database_name}").collect()
tables=[table["tableName"] for table in tables]

# Filter tables starting with "staging"
# staging_tables = [table["tableName"] for table in tables if table["tableName"].startswith("staging_")]

results=[]

# Calculate row count for each of the tables and append the results into an array with table name and row count:
for table_name in tables:
    # full_table_name=f"{database_name}.{table_name}"
    # trimmed_table_name=table_name[8:]
    count=spark.table(table_name).count()
    results.append((table_name,count))
    
# Create dataframe form the array:
results_df = spark.createDataFrame(results,["TableName","RowCount"])\
    .withColumn('RowCount',format_number('RowCount',0))


# Splitting Table Name to create more readable report of row count in the Lakehouse
split_col = split(results_df['TableName'], 'SPLITTER')
trimmed_df = results_df.withColumn('Stage', split_col.getItem(0))\
    .withColumn('TableName', split_col.getItem(1))\
    .orderBy(col('TableName').asc())\
    .select('Stage','TableName','RowCount')
    
display(trimmed_df)

***********************************************************************************************************************************************************************************************************************
**************************************************** THOUSANDS SEPARATOR ISSUE ****************************************************************************************************************************************
from pyspark.sql import SparkSession
from pyspark.sql.functions import split, col, when, length, expr

# MULTIPLE COLUMNS THAT START WITH DAY_
day_columns = [col_name for col_name in df.columns if col_name.startswith('DAY')]


def process_decimal_column(col_name):
    parts = split(col(col_name), "\.")
    is_negative = col(col_name).startswith("-")
    no_decimal = parts.getItem(1).isNull()
    
    before_dot = when(is_negative,parts.getItem(0).cast("int") * -1000)\
                 .otherwise(parts.getItem(0).cast("int") * 1000)
                 
    after_dot_value = when(length(parts.getItem(1)) >= 3, parts.getItem(1).substr(1,3).cast("int"))\
                      .when(length(parts.getItem(1)) == 1, parts.getItem(1).cast("int") * 100)\
                      .when(length(parts.getItem(1)) == 2, parts.getItem(1).cast("int") * 10)\
                      .otherwise(parts.getItem(1).cast("int"))

    
    after_dot = when(is_negative, -1 * after_dot_value).otherwise(after_dot_value)
    
    # Combining the before and after dot parts based on negativity
    result = when(no_decimal, col(col_name)).otherwise(before_dot + after_dot)
    
    return result



# Using a for loop to apply the function to each 'DAY' column
for col_name in day_columns:
    df = df.withColumn(col_name + "_processed", process_decimal_column(col_name))
# Select the modified columns along with any other columns you need
# df = df.select(*modifications)

# Show the updated DataFrame

display(df)

*************************************************************************************************************************************************************************************************************************
********************************************** NEW CODE FOR CREATING SILVER TABLE IN PIPELINE RUN ***********************************************************************************************************************
from pyspark.sql.functions import *

# Change the name of tables if necessary: staging table is from pipeline, Silver table is adjusted table, Grouped table is table for teporting and only one for all tables:
var_StagingName = "staging_HUD_"+var_TableName
var_TableNameSilver = "silver_HUD_"+var_TableName
var_TableNameGrouped = "report_HUD_load"
# var_Timestamp = current_timestamp()

# reading the staging table from the pipeline and adding ExtractTimestampGMT to the dataframe with current timestamp and saving the table as lkh_silver_ table:
df=spark.read.table(var_StagingName)
# number of rows loaded:
var_dfCount = df.count()
# adding ExtractTimestampUTC to staging table and save it as silver table:
df.withColumn('ExtractTimestampUTC',lit(var_StartTime))\
    .write.mode("overwrite").format("delta").option("overwriteSchema", "true").save(f"Tables/{var_TableNameSilver}")

# UPDATING REPORTING TABLE
# creating Dataframe from variables and schema:
var_Duration = ""
report_data=[(var_StartTime,var_WorkspaceID,var_PipelineName,var_PipelineRunID,var_TableName,var_dfCount,var_StartTime,var_EndTime,var_Duration)]
columns=["ExtractDate","WorkspaceID","PipelineName","PipelineRunID","TableName","NumOfRowsLoaded","CopyActivityStartTime","CopyActivityEndTime","CopyActivityDuration_sec"]

# creating dataframe and adjusting the column format and calculating duration of load:
df=spark.createDataFrame(report_data,columns)\
    .withColumn('ExtractDate', to_date('ExtractDate'))\
    .withColumn('CopyActivityStartTime',date_format('CopyActivityStartTime','HH:mm:ss'))\
    .withColumn('CopyActivityEndTime',date_format('CopyActivityEndTime','HH:mm:ss'))\
    .withColumn('CopyActivityStartTime',to_timestamp('CopyActivityStartTime'))\
    .withColumn('CopyActivityEndTime',to_timestamp('CopyActivityEndTime'))\
    .withColumn('CopyActivityDuration_sec',unix_timestamp('CopyActivityEndTime')-unix_timestamp('CopyActivityStartTime'))

# Appending the dataframe to the report table:
df.write.mode("append").format("delta").partitionBy("ExtractDate").save(f"Tables/{var_TableNameGrouped}")

# Deleting staging table:
spark.sql(f"DROP TABLE IF EXISTS {var_StagingName}")
**************************************************************************************************************************************************************************************************************************
********************************************************* COMPARING MIN AND MAX VALUES OF TWO TABLES IN ONE COLUMN, CUNTING ROWS *****************************************************************************************

from pyspark.sql.functions import *

# comparing stat for two tables (max, min values for date columns; row count) into one report table:

var_ARtable = "staging_HUD_ACCOUNTS_RECEIVABLE_SUBLEDGER_HARMONISED_closed"
var_INCRtable = "staging_AR_Closed_INCR_Test"
var_Timestamp = current_timestamp()
incr_column_name = 'MARC_secondary_reference_HLoadTS'

tables = [var_ARtable,var_INCRtable]
results=[]

for table_name in tables:
    ar_df = spark.read.table(table_name).select(incr_column_name) # just loading one columne for each table, we are only interested for this one
    ar_df = ar_df.withColumn(incr_column_name,to_date(incr_column_name)) # to have column for each table in the same data type
    minHLoadTS_row = ar_df.agg(min(incr_column_name).alias('MinValue')).first() # finding min value, result is dataframe
    maxHLoadTS_row = ar_df.agg(max(incr_column_name).alias('MaxValue')).first() # finding max value, result is dataframe
    min_value=minHLoadTS_row['MinValue'] # turning into value from dataframe
    max_value=maxHLoadTS_row['MaxValue'] # turning into value from dataframe
    row_count = ar_df.count() #counting number of rows
    results.append((table_name,min_value,max_value,row_count)) # appending each result for each table into array

# creating reporting dataframe:

columns_ar = ["TableName","minHLoadTS","maxHLoadTS","RowCount"]
results_df=spark.createDataFrame(results,columns_ar).withColumn('ReportDate',lit(var_Timestamp))

results_df.write.mode("append").format("delta").save(f"Tables/IncrementalReporting")
